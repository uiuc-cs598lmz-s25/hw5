{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d446fbf",
   "metadata": {},
   "source": [
    "# LLM Agent for Software Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f8aedef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import locale\n",
    "locale.getpreferredencoding = lambda: \"UTF-8\" # this is needed to get rid of weird colab locale error\n",
    "# if you are still running into issues, please restart the runtime to initialize a new environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d8a965",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/evalplus/evalplus\n",
    "!pip install evalplus==0.2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3f17bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install litellm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da157c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tenacity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84da988c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/uiuc-cs598lmz-s25/hw5/main/buggy_humaneval.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0340644d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def grab_buggy_dataset():\n",
    "    inference_dataset = []\n",
    "    file = \"buggy_humaneval.jsonl\"\n",
    "    with open(file, \"r\") as f:\n",
    "        inference_dataset.extend([json.loads(x) for x in f.readlines()])\n",
    "    print(\"Number of tasks: {}\".format(len(inference_dataset)))\n",
    "    return inference_dataset\n",
    "\n",
    "buggy_humaneval = grab_buggy_dataset()\n",
    "# feel free to play around the dataset for a bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d169c348",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Basic Tool Class ###\n",
    "# Here we define a basic tool class that can be extended to create custom tools.\n",
    "from abc import ABC, ABCMeta, abstractmethod\n",
    "from dataclasses import dataclass\n",
    "from litellm import ChatCompletionToolParam, ChatCompletionToolParamFunctionChunk\n",
    "from typing import Any\n",
    "\n",
    "@dataclass(kw_only=True, frozen=True)\n",
    "class ToolResult:\n",
    "    \"\"\"Represents the result of a tool execution.\"\"\"\n",
    "    output: str | None = None  # This is the response that will be provided to the LLM.\n",
    "    data: dict[str, Any] | None = None  # Additional data that might be returned, not shown to the LLM.\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.output\n",
    "\n",
    "\n",
    "class ToolError(ToolResult):\n",
    "    \"\"\"Raised when a tool encounters an error.\"\"\"\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Error: {self.output}\"\n",
    "\n",
    "\n",
    "class Tool(ABC):\n",
    "    \"\"\"Abstract class for tools.\"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    def call(self, *args, **kwargs) -> ToolResult:\n",
    "        \"\"\"Execute the tool\"\"\"\n",
    "        ...\n",
    "\n",
    "    def __call__(self, *args, **kwargs) -> ToolResult:\n",
    "        return self.call(*args, **kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34caad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Wrapper for LLM ###\n",
    "# we use the litellm framework for easier model access\n",
    "import litellm\n",
    "import os\n",
    "from litellm import completion, ModelResponse\n",
    "from typing import Literal\n",
    "\n",
    "os.environ[\"GEMINI_API_KEY\"] = \"YOUR_API_KEY_HERE\"\n",
    "\n",
    "class LLM(ABCMeta):\n",
    "\n",
    "    @abstractmethod\n",
    "    def completion(self, messages: list, tools: list) -> ModelResponse:\n",
    "        \"\"\"Get the completion from the LLM\"\"\"\n",
    "        ...\n",
    "\n",
    "def _completion_with_retries(*args, **kwargs):\n",
    "    \"\"\"\n",
    "    Executes a litellm.completion() with retries\n",
    "    \"\"\"\n",
    "    try:\n",
    "        import tenacity\n",
    "    except Exception as e:\n",
    "        raise Exception(\n",
    "            f\"tenacity import failed please run `pip install tenacity`. Error{e}\"\n",
    "        )\n",
    "\n",
    "    kwargs[\"max_retries\"] = 0\n",
    "    kwargs[\"num_retries\"] = 0\n",
    "    retry_strategy: Literal[\"exponential_backoff_retry\", \"constant_retry\"] = kwargs.pop(\n",
    "        \"retry_strategy\", \"constant_retry\"\n",
    "    )  # type: ignore\n",
    "    original_function = kwargs.pop(\"original_function\", completion)\n",
    "    if retry_strategy == \"exponential_backoff_retry\":\n",
    "        retryer = tenacity.Retrying(\n",
    "            wait=tenacity.wait_exponential(multiplier=1, min=20, max=120),\n",
    "            reraise=True,\n",
    "        )\n",
    "    else:\n",
    "        retryer = tenacity.Retrying(\n",
    "            wait=tenacity.wait_fixed(20),\n",
    "            retry=tenacity.retry_if_exception_type(\n",
    "                (litellm.ContentPolicyViolationError, litellm.Timeout, litellm.RateLimitError, )\n",
    "            ),\n",
    "            reraise=True,\n",
    "        )\n",
    "    return retryer(original_function, *args, **kwargs)\n",
    "\n",
    "class GeminiLLM():\n",
    "    def __init__(self, model_name: str, temperature: float = 1.0, max_tokens: int = 512):\n",
    "        self.model_name = model_name\n",
    "        self.temperature = temperature\n",
    "        self.max_tokens = max_tokens\n",
    "        # similar to the vulnerability detection assignment, we adjust the level of blocking for gemini\n",
    "        # to make it less strict, we set the threshold to BLOCK_NONE\n",
    "        self.safety_settings = [\n",
    "            {\n",
    "                \"category\": \"HARM_CATEGORY_CIVIC_INTEGRITY\",\n",
    "                \"threshold\": \"BLOCK_NONE\",\n",
    "            },\n",
    "            {\n",
    "                \"category\": \"HARM_CATEGORY_HARASSMENT\",\n",
    "                \"threshold\": \"BLOCK_NONE\",\n",
    "            },\n",
    "            {\n",
    "                \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n",
    "                \"threshold\": \"BLOCK_NONE\",\n",
    "            },\n",
    "            {\n",
    "                \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n",
    "                \"threshold\": \"BLOCK_NONE\",\n",
    "            },\n",
    "            {\n",
    "                \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
    "                \"threshold\": \"BLOCK_NONE\",\n",
    "            },\n",
    "        ]\n",
    "\n",
    "    def completion(self, messages: list, tools: list = None) -> ModelResponse:\n",
    "        response = _completion_with_retries(\n",
    "            model=self.model_name,\n",
    "            messages=messages,\n",
    "            safety_settings=self.safety_settings,\n",
    "            tools=tools,\n",
    "            temperature=self.temperature,\n",
    "            max_tokens=self.max_tokens,\n",
    "        )\n",
    "        return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ebd8e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Example Usage ###\n",
    "gemini = GeminiLLM(model_name=\"gemini/gemini-2.0-flash\", temperature=1.0, max_tokens=10)\n",
    "model_response = gemini.completion(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"What is 1 + 2?\"\n",
    "        }\n",
    "    ],\n",
    ")\n",
    "print(model_response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e72a737",
   "metadata": {},
   "source": [
    "## Localization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44be2428",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrintBuggyLocalizationTool(Tool):\n",
    "    \"\"\"A tool that prints the buggy line in a code snippet.\"\"\"\n",
    "    tool_description = \"\"\"A tool that prints the buggy line in a given code snippet.\"\"\"\n",
    "    tool_param_dict = {\n",
    "        \"name\": \"print_buggy_line\",\n",
    "        \"description\": tool_description,\n",
    "        \"input_schema\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"line_number\": {\n",
    "                    \"description\": \"The line number of the buggy line in the code snippet (1-indexed).\",\n",
    "                    \"type\": \"integer\",\n",
    "                },\n",
    "            },\n",
    "            \"required\": [\"line_number\"],\n",
    "        },\n",
    "    }\n",
    "\n",
    "    def __init__(self, ):\n",
    "        self.tool_param = ChatCompletionToolParam(\n",
    "            type=\"function\",\n",
    "            function=ChatCompletionToolParamFunctionChunk(\n",
    "                name=self.tool_param_dict[\"name\"],\n",
    "                description=self.tool_param_dict[\"description\"],\n",
    "                parameters=self.tool_param_dict[\"input_schema\"],\n",
    "            )\n",
    "        )\n",
    "\n",
    "\n",
    "    def call(self, line_number: int, buggy_code: str) -> ToolResult:\n",
    "        \"\"\"Call the tool with the given parameters.\"\"\"\n",
    "        # TODO: Implement the logic to call the tool\n",
    "        # Note that LLM can generate invalid argument (e.g., line_number can be out of bound)\n",
    "        # Please first validate that the line_number is valid, and return ToolError with a proper error message if it is not.\n",
    "        # Next, please extract the buggy line from the code snippet and return in the output field of the tool result.\n",
    "        return ToolError(output=\"Tool not implemented yet\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6d703d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this assignment we implement a simple localization agent to locate the buggy line and report the localization accuracy\n",
    "\n",
    "class Agent(metaclass=ABCMeta):\n",
    "\n",
    "\n",
    "    def get_tool(self, tool_name: str) -> Tool:\n",
    "        \"\"\"Retrieve a tool by its name.\"\"\"\n",
    "        for tool in self.tools:\n",
    "            if tool.tool_param[\"function\"][\"name\"] == tool_name:\n",
    "                return tool\n",
    "\n",
    "        raise ValueError(f\"Tool {tool_name} not found.\")\n",
    "\n",
    "\n",
    "\n",
    "class LocalizationAgent(Agent):\n",
    "\n",
    "    def __init__(self, buggy_code: str, llm: LLM):\n",
    "        self.buggy_code = buggy_code\n",
    "        self.tools = [PrintBuggyLocalizationTool(), ]\n",
    "        self.llm = llm\n",
    "\n",
    "    def build_user_message(self) -> str:\n",
    "        \"\"\"Build the initial user message for the LLM.\"\"\"\n",
    "        # TODO: Implement the logic to construct the initial prompt with the buggy code.\n",
    "        # To make it easier for the LLM to locate the buggy line, don't forget to add line numbers to the code.\n",
    "        \n",
    "        return user_message\n",
    "\n",
    "    def run(self,) -> int | None:\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": self.build_user_message(),\n",
    "            },\n",
    "        ]\n",
    "        tools = [tool.tool_param for tool in self.tools]\n",
    "        response = self.llm.completion(messages, tools)\n",
    "\n",
    "\n",
    "        # TODO: process response and parse the tool call into a line number\n",
    "        # if the response does not contain a valid tool call, return None\n",
    "        # Hint: you can use `response.choices[0].message.tool_calls` to get the tool calls\n",
    "        # Hint: you can use `json.loads()` to parse the tool call arguments\n",
    "        # Hint: note that the line number is 1-indexed, so you need to convert it to 0-indexed\n",
    "\n",
    "        return line_number\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47138a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Test the Localization Agent on a single task ###\n",
    "gemini = GeminiLLM(model_name=\"gemini/gemini-2.0-flash\", temperature=1.0, max_tokens=512)\n",
    "localization_agent = LocalizationAgent(\n",
    "    buggy_code=buggy_humaneval[0]['buggy_code'],\n",
    "    llm=gemini\n",
    ")\n",
    "pred_line_number = localization_agent.run()\n",
    "print(\"pred line number = \", pred_line_number)\n",
    "print(\"gt line number = \", buggy_humaneval[0]['buggy_line'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135e8324",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def gemini_fault_localization(model, bug_dataset, workdir) -> tuple[float, float]:\n",
    "\n",
    "    all_ids = []\n",
    "    complete_ids = []\n",
    "    pass_ids = []\n",
    "  \n",
    "    os.makedirs(workdir, exist_ok=True)\n",
    "    with open(os.path.join(workdir, \"buggy_line_predictions.jsonl\"), \"w\") as f:\n",
    "        f.write(\"\")\n",
    "    for bug in tqdm(bug_dataset):\n",
    "        localization_agent = LocalizationAgent(\n",
    "            buggy_code=bug['buggy_code'],\n",
    "            llm=model\n",
    "        )\n",
    "    gt_line_number = bug['buggy_line']\n",
    "    pred_line_number = localization_agent.run()\n",
    "    with open(os.path.join(workdir, \"buggy_line_predictions.jsonl\"), \"a\") as f:\n",
    "        f.write(json.dumps({\n",
    "            \"task_id\": bug['task_id'],\n",
    "            \"gt_line_number\": gt_line_number,\n",
    "            \"pred_line_number\": pred_line_number,\n",
    "        }) + \"\\n\")\n",
    "    \n",
    "    if pred_line_number:\n",
    "        complete_ids.append(bug['task_id'])\n",
    "    if pred_line_number and pred_line_number == gt_line_number:\n",
    "        pass_ids.append(bug['task_id'])\n",
    "    all_ids.append(bug['task_id'])\n",
    "    accuracy = len(pass_ids) / len(all_ids)\n",
    "    complete_rate = len(complete_ids) / len(all_ids)\n",
    "    return accuracy, complete_rate\n",
    "\n",
    "# accuracy of gemini\n",
    "gemini_acc_score, gemini_complete_rate = gemini_fault_localization(gemini, buggy_humaneval, \"gemini_localization\")\n",
    "print(f\"{gemini_acc_score = }, {gemini_complete_rate = }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "029f0ded",
   "metadata": {},
   "source": [
    "## Repair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd7a9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextEditorTool(Tool):\n",
    "    \"\"\"A tool that edits a code snippet.\"\"\"\n",
    "    # TODO: Implement the tool definition\n",
    "    # Hint: you can write a long description for the tool to help the LLM understand how to use it\n",
    "    # Hint: valid type of input parameters are: \"integer\", \"string\", etc\n",
    "    # Hint: don't forget to set the required parameters for the tool\n",
    "    tool_param_dict = ...\n",
    "\n",
    "    def __init__(self):\n",
    "        self.tool_param = ChatCompletionToolParam(\n",
    "            type=\"function\",\n",
    "            function=ChatCompletionToolParamFunctionChunk(\n",
    "                name=self.tool_param_dict[\"name\"],\n",
    "                description=self.tool_param_dict[\"description\"],\n",
    "                parameters=self.tool_param_dict[\"input_schema\"],\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    def run_command(self, code, command: str, line_number: int = None, old_str: str = None, new_str: str = None) -> str:\n",
    "        \"\"\"Run the specified command on the code, and return the edited code.\"\"\"\n",
    "        # TODO: Implement the logic to run the command on the code\n",
    "        # Hint: perform different operations based on the command\n",
    "        # Hint: you may raise ValueError if missing required parameters or invalid command\n",
    "        return new_code\n",
    "\n",
    "    def call(self, code, command: str, line_number: int = None, old_str: str = None, new_str: str = None) -> ToolResult:\n",
    "        try:\n",
    "            new_code = self.run_command(code, command, line_number, old_str, new_str)\n",
    "            return ToolResult(output=f\"The code has been successfully edited. Below is the new code:\\n```python\\n{new_code}\\n```\",\n",
    "                              data={\"new_code\": new_code})\n",
    "        except Exception as e:\n",
    "            return ToolError(output=f\"Error: {str(e)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e6c242",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleRepairAgent(Agent):\n",
    "\n",
    "    def __init__(self, buggy_code: str, llm: LLM):\n",
    "        self.buggy_code = buggy_code\n",
    "        self.tools = [TextEditorTool(), ]\n",
    "        self.llm = LLM\n",
    "    \n",
    "    def build_user_message(self) -> str:\n",
    "        \"\"\"Build the initial user message for the LLM.\"\"\"\n",
    "        # TODO: Implement the logic to construct the initial prompt with the buggy code.\n",
    "\n",
    "    def run(self) -> str:\n",
    "        # TODO: Implement the logic to run the agent\n",
    "        return fixed_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e08864b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import difflib\n",
    "def get_diff(old_code: str, new_code: str) -> str:\n",
    "    \"\"\"Compute the difference between two code snippets.\"\"\"\n",
    "    diff = difflib.unified_diff(old_code.splitlines(), new_code.splitlines(), fromfile=\"old\", tofile=\"new\", lineterm=\"\")\n",
    "    return \"\\n\".join(diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3113e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Test the Repair Agent on a single task ###\n",
    "gemini = GeminiLLM(model_name=\"gemini/gemini-2.0-flash\", temperature=1.0, max_tokens=512)\n",
    "repair_agent = SimpleRepairAgent(\n",
    "    buggy_code=buggy_humaneval[0]['buggy_code'],\n",
    "    llm=gemini\n",
    ")\n",
    "fixed_code = repair_agent.run()\n",
    "print(\"========= Fixed Code =========\")\n",
    "print(fixed_code)\n",
    "print(\"========= DIFF =========\")\n",
    "print(get_diff(buggy_humaneval[0]['buggy_code'], fixed_code))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe57172",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def gemini_repair(model, bug_dataset, workdir) -> float:\n",
    "\n",
    "  for bug in tqdm(bug_dataset):\n",
    "    repair_agent = SimpleRepairAgent(\n",
    "        buggy_code=bug['buggy_code'],\n",
    "        llm=model\n",
    "    )\n",
    "    # run the repair agent\n",
    "    fixed_code = repair_agent.run()\n",
    "\n",
    "    name = bug[\"task_id\"].replace(\"/\", \"_\")\n",
    "    os.makedirs(os.path.join(workdir, name), exist_ok=True)\n",
    "    with open(os.path.join(workdir, name, '0.py'), 'w') as f:\n",
    "        f.write(fixed_code)\n",
    "\n",
    "gemini_repair(gemini, buggy_humaneval, \"gemini_repair\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d64cf6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!yes Y | evalplus.evaluate --dataset humaneval --samples gemini_repair --i-just-wanna-run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f38e69a",
   "metadata": {},
   "source": [
    "## Repair with test execution feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ec2216",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evalplus.data import get_human_eval_plus\n",
    "human_eval_plus = get_human_eval_plus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181810a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RunTestTool(Tool):\n",
    "    \"\"\"A tool that executes a function for a test input.\"\"\"\n",
    "    # TODO: Implement the tool definition\n",
    "    \n",
    "    def __init__(self, canonical_solution: str = None):\n",
    "        self.tool_param = ChatCompletionToolParam(\n",
    "            type=\"function\",\n",
    "            function=ChatCompletionToolParamFunctionChunk(\n",
    "                name=self.tool_param_dict[\"name\"],\n",
    "                description=self.tool_param_dict[\"description\"],\n",
    "                parameters=self.tool_param_dict[\"input_schema\"],\n",
    "            )\n",
    "        )\n",
    "        self.canonical_solution = canonical_solution\n",
    "    \n",
    "    def _execute(self, function_definition_code: str, function_invocation: str):\n",
    "        \"\"\"Execute the function with the given code and function invocation.\"\"\"\n",
    "        exec_globals = {}\n",
    "        exec_locals = {}\n",
    "        exec(function_definition_code, exec_globals, exec_locals)\n",
    "        result = eval(f\"{function_invocation}\", exec_globals, exec_locals)\n",
    "        return result\n",
    "\n",
    "    def call(self, code: str, function_invocation) -> ToolResult:\n",
    "        \"\"\"Run the tests on the code snippet.\"\"\"\n",
    "        expected_output_message = \"\"\n",
    "        if self.canonical_solution:\n",
    "            try:\n",
    "                gold_result = self._execute(self.canonical_solution, function_invocation)\n",
    "                expected_output_message = f\"\\n\\nExpected output:\\n```\\n{gold_result}\\n```\"\n",
    "            except Exception as e:\n",
    "                return ToolError(output=f\"Error: The test input is not valid.\\n{str(e)}\")\n",
    "        try:\n",
    "            result = self._execute(code, function_invocation)\n",
    "            return ToolResult(output=f\"Execution output:\\n```\\n{result}\\n```\" + expected_output_message,\n",
    "                              data={\"result\": result})\n",
    "        except Exception as e:\n",
    "            return ToolError(output=f\"Error: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0a72be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function to dump message to JSON\n",
    "from json import JSONEncoder\n",
    "\n",
    "\n",
    "class MessageEncoder(JSONEncoder):\n",
    "        def default(self, o):\n",
    "            return o.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afbb3aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RepairAgent(Agent):\n",
    "\n",
    "    def __init__(self, buggy_code: str, llm: LLM, canonical_solution: str = None):\n",
    "        self.buggy_code = buggy_code\n",
    "        self.tools = [TextEditorTool(), RunTestTool(canonical_solution=canonical_solution), ]\n",
    "        self.llm = llm\n",
    "        self.canonical_solution = canonical_solution\n",
    "    \n",
    "    def build_user_message(self) -> str:\n",
    "        # TODO: construct the initial prompt with the buggy code\n",
    "\n",
    "    def run(self) -> str:\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": self.build_user_message(),\n",
    "            },\n",
    "        ]\n",
    "        current_code = self.buggy_code\n",
    "\n",
    "        # Set a maximum number of iterations to prevent infinite loops\n",
    "        for _ in range(20):\n",
    "            tools = [tool.tool_param for tool in self.tools]\n",
    "            response = self.llm.completion(messages, tools)\n",
    "\n",
    "            model_message = response.choices[0].message\n",
    "            messages.append(model_message.model_dump())\n",
    "            # TODO: Implement the logic to parse and run the tool calls\n",
    "            # Hint: You can terminate the loop if the model does not return any tool calls\n",
    "            # Hint: You may use `self.get_tool(tool_name)` to get the tool\n",
    "            # Hint: After you execute the tool, you need to update the current code\n",
    "            # Hint: After you get the tool result, you need to append a tool message to the message list\n",
    "            # The tool message is in the following format:\n",
    "            # {\n",
    "            #     \"role\": \"tool\",\n",
    "            #     \"tool_call_id\": tool_call.id,\n",
    "            #     \"content\": tool_result.output,\n",
    "            #     \"name\": the function name of the tool call,\n",
    "            # }\n",
    "        return current_code, messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b24a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "gemini = GeminiLLM(model_name=\"gemini/gemini-2.0-flash\", temperature=1.0, max_tokens=512)\n",
    "task_id = buggy_humaneval[0]['task_id']\n",
    "canonical_solution = human_eval_plus[task_id][\"prompt\"] + human_eval_plus[task_id][\"canonical_solution\"]\n",
    "repair_agent = RepairAgent(\n",
    "    buggy_code=buggy_humaneval[0]['buggy_code'],\n",
    "    llm=gemini,\n",
    "    canonical_solution=canonical_solution,\n",
    ")\n",
    "fixed_code, messages = repair_agent.run()\n",
    "print(\"========= Fixed Code =========\")\n",
    "print(fixed_code)\n",
    "print(\"========= DIFF =========\")\n",
    "print(get_diff(buggy_humaneval[0]['buggy_code'], fixed_code))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11167c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def gemini_repair(model, bug_dataset, workdir, restart=True) -> float:\n",
    "\n",
    "    for bug in tqdm(bug_dataset):\n",
    "        name = bug[\"task_id\"].replace(\"/\", \"_\")\n",
    "        if not restart:\n",
    "            if os.path.exists(os.path.join(workdir, name, '0.py')):\n",
    "                continue\n",
    "        task_id = bug['task_id']\n",
    "        canonical_solution = human_eval_plus[task_id][\"prompt\"] + human_eval_plus[task_id][\"canonical_solution\"]\n",
    "        repair_agent = RepairAgent(\n",
    "            buggy_code=bug['buggy_code'],\n",
    "            llm=model,\n",
    "            canonical_solution=canonical_solution,\n",
    "        )\n",
    "        # run the repair agent\n",
    "        fixed_code, messages = repair_agent.run()\n",
    "\n",
    "        name = bug[\"task_id\"].replace(\"/\", \"_\")\n",
    "        os.makedirs(os.path.join(workdir, name), exist_ok=True)\n",
    "        with open(os.path.join(workdir, name, '0.py'), 'w') as f:\n",
    "            f.write(fixed_code)\n",
    "        with open(os.path.join(workdir, name, 'traj.json'), 'w') as f:\n",
    "            json.dump(messages, f, indent=2, cls=MessageEncoder)\n",
    "\n",
    "\n",
    "gemini_repair(gemini, buggy_humaneval, \"gemini_repair_with_execution_feedback\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2443cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!yes Y | evalplus.evaluate --dataset humaneval --samples gemini_repair_with_execution_feedback --i-just-wanna-run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4006ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
