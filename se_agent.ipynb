{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d446fbf",
   "metadata": {},
   "source": [
    "# LLM Agent for Software Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f8aedef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import locale\n",
    "locale.getpreferredencoding = lambda: \"UTF-8\" # this is needed to get rid of weird colab locale error\n",
    "# if you are still running into issues, please restart the runtime to initialize a new environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d8a965",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/evalplus/evalplus\n",
    "!pip install evalplus==0.2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84da988c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/uiuc-cs598lmz-s25/hw5/main/buggy_humaneval.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0340644d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def grab_buggy_dataset():\n",
    "    inference_dataset = []\n",
    "    file = \"buggy_humaneval.jsonl\"\n",
    "    with open(file, \"r\") as f:\n",
    "        inference_dataset.extend([json.loads(x) for x in f.readlines()])\n",
    "    print(\"Number of tasks: {}\".format(len(inference_dataset)))\n",
    "    return inference_dataset\n",
    "\n",
    "buggy_humaneval = grab_buggy_dataset()\n",
    "# feel free to play around the dataset for a bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d169c348",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Basic Tool Result Class ###\n",
    "# Here we define a basic tool result class\n",
    "import os\n",
    "from abc import ABC, ABCMeta, abstractmethod\n",
    "from dataclasses import dataclass\n",
    "from typing import Any\n",
    "\n",
    "@dataclass(kw_only=True, frozen=True)\n",
    "class ToolResult:\n",
    "    \"\"\"Represents the result of a tool execution.\"\"\"\n",
    "    output: str\n",
    "    data: dict[str, Any] = None\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.output\n",
    "\n",
    "\n",
    "class ToolError(ToolResult):\n",
    "    \"\"\"Raised when a tool encounters an error.\"\"\"\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Error: {self.output}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c21a324",
   "metadata": {},
   "source": [
    "### Gemini Model Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fba8f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "from google.genai import types\n",
    "client = genai.Client(api_key=\"YOUR_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3425c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import signal\n",
    "import time\n",
    "\n",
    "safety_settings = [ # google is afraid of pretty much everything i think.\n",
    "    {\n",
    "        \"category\": \"HARM_CATEGORY_CIVIC_INTEGRITY\",\n",
    "        \"threshold\": \"BLOCK_NONE\",\n",
    "    },\n",
    "    {\n",
    "        \"category\": \"HARM_CATEGORY_HARASSMENT\",\n",
    "        \"threshold\": \"BLOCK_NONE\",\n",
    "    },\n",
    "    {\n",
    "        \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n",
    "        \"threshold\": \"BLOCK_NONE\",\n",
    "    },\n",
    "    {\n",
    "        \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n",
    "        \"threshold\": \"BLOCK_NONE\",\n",
    "    },\n",
    "    {\n",
    "        \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
    "        \"threshold\": \"BLOCK_NONE\",\n",
    "    },\n",
    "]\n",
    "\n",
    "\n",
    "def create_gemini_config(\n",
    "    max_tokens: int,\n",
    "    temperature: float = 1,\n",
    "    batch_size: int = 1,\n",
    "    tools: list = None,\n",
    "):\n",
    "    config = types.GenerateContentConfig(\n",
    "        candidate_count=batch_size,\n",
    "        max_output_tokens=max_tokens,\n",
    "        temperature=temperature,\n",
    "        safety_settings=safety_settings,\n",
    "        tools=tools,\n",
    "    )\n",
    "    return config\n",
    "\n",
    "\n",
    "def handler(signum, frame):\n",
    "    # swallow signum and frame\n",
    "    raise Exception(\"end of time\")\n",
    "\n",
    "\n",
    "def request_gemini_engine(chat_client, message, config):\n",
    "    ret = None\n",
    "    count = 0\n",
    "    while ret is None:\n",
    "        try:\n",
    "            signal.signal(signal.SIGALRM, handler)\n",
    "            signal.alarm(100)\n",
    "            ret = chat_client.send_message(message, config=config)\n",
    "            s = ret.candidates  # check if response can be accessed.\n",
    "            signal.alarm(0)\n",
    "        except Exception as e:\n",
    "            # NOTE this exception handling is needed since sometimes gemini will\n",
    "            # refuse to answer due to safety reason (even if all blockers are set off)\n",
    "            # instead we just simply catch this and then retry the response.\n",
    "            # don't be alarmed if certain inputs take a long time to finish\n",
    "            # eventually it should return a response.\n",
    "            print(\"Error\", type(e), e)\n",
    "            \n",
    "            ret = None  # reset\n",
    "            signal.alarm(0)\n",
    "            time.sleep(20)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e72a737",
   "metadata": {},
   "source": [
    "## Localization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44be2428",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print_buggy_location_tool_definition = {\n",
    "    \"name\": \"print_buggy_line\",\n",
    "    \"description\": \"\"\"A tool that prints the buggy line in a given code snippet.\"\"\",\n",
    "    \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"line_number\": {\n",
    "                \"description\": \"The line number of the buggy line in the code snippet (1-indexed).\",\n",
    "                \"type\": \"integer\",\n",
    "            },\n",
    "        },\n",
    "        \"required\": [\"line_number\"],\n",
    "    },\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6d703d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this assignment we implement a simple localization agent to locate the buggy line and report the localization accuracy\n",
    "\n",
    "def run_localization_agent(buggy_code, client) -> int | None:\n",
    "    \"\"\"Run the localization agent to find the buggy line.\"\"\"\n",
    "    chat = client.chats.create(model=\"models/gemini-2.0-flash\")\n",
    "    # TODO: Implement the logic to construct the initial prompt with the buggy code.\n",
    "    # To make it easier for the LLM to locate the buggy line, don't forget to add line numbers to the code.\n",
    "    user_message = \"...\"\n",
    "    config = create_gemini_config(\n",
    "        max_tokens=512,\n",
    "        temperature=1.0,\n",
    "        batch_size=1,\n",
    "        # we pass the tools through the generation config\n",
    "        tools=[types.Tool(functionDeclarations=[print_buggy_location_tool_definition])],\n",
    "    )\n",
    "    response = request_gemini_engine(chat, user_message, config)\n",
    "\n",
    "    if not response.function_calls:\n",
    "        return None\n",
    "    line_number = None\n",
    "    for function_call in response.function_calls:\n",
    "        func_name = function_call.name\n",
    "        func_args = function_call.args\n",
    "\n",
    "        # TODO: process response and parse the tool call into a line number\n",
    "        ...\n",
    "\n",
    "\n",
    "    # If the response does not contain a valid tool call, return None\n",
    "    # If the response contains a valid tool call, return the predicted buggy line number (0-indexed)\n",
    "    # 0-index means the first line of the code is line 0. For example, if the 5th line is buggy, the agent shall return an integer 4\n",
    "    # This is consistent with the `buggy_humaneval.jsonl` dataset, where the `buggy_line` field is 0-indexed\n",
    "    return line_number\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47138a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Test the Localization Agent on a single task ###\n",
    "pred_line_number = run_localization_agent(\n",
    "    buggy_code=buggy_humaneval[0]['buggy_code'],\n",
    "    client=client\n",
    ")\n",
    "print(\"pred line number = \", pred_line_number)\n",
    "print(\"gt line number = \", buggy_humaneval[0]['buggy_line'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135e8324",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def gemini_fault_localization(client, bug_dataset, workdir) -> tuple[float, float]:\n",
    "\n",
    "    all_ids = []\n",
    "    complete_ids = []\n",
    "    pass_ids = []\n",
    "  \n",
    "    os.makedirs(workdir, exist_ok=True)\n",
    "    with open(os.path.join(workdir, \"buggy_line_predictions.jsonl\"), \"w\") as f:\n",
    "        f.write(\"\")\n",
    "    for bug in tqdm(bug_dataset):\n",
    "        gt_line_number = bug['buggy_line']\n",
    "        pred_line_number = run_localization_agent(\n",
    "            buggy_code=bug['buggy_code'],\n",
    "            client=client\n",
    "        )\n",
    "        with open(os.path.join(workdir, \"buggy_line_predictions.jsonl\"), \"a\") as f:\n",
    "            f.write(json.dumps({\n",
    "                \"task_id\": bug['task_id'],\n",
    "                \"gt_line_number\": gt_line_number,\n",
    "                \"pred_line_number\": pred_line_number,\n",
    "            }) + \"\\n\")\n",
    "        \n",
    "            if pred_line_number is not None:\n",
    "                complete_ids.append(bug['task_id'])\n",
    "            if pred_line_number is not None and pred_line_number == gt_line_number:\n",
    "                pass_ids.append(bug['task_id'])\n",
    "            all_ids.append(bug['task_id'])\n",
    "    accuracy = len(pass_ids) / len(all_ids)\n",
    "    complete_rate = len(complete_ids) / len(all_ids)\n",
    "    return accuracy, complete_rate\n",
    "\n",
    "# accuracy of gemini\n",
    "gemini_acc_score, gemini_complete_rate = gemini_fault_localization(client, buggy_humaneval, \"gemini_localization\")\n",
    "print(f\"{gemini_acc_score = }, {gemini_complete_rate = }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "029f0ded",
   "metadata": {},
   "source": [
    "## Repair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd7a9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Definition of the TextEditorTool\n",
    "\n",
    "# TODO: Implement the tool definition\n",
    "text_editor_tool_definition = {\n",
    "    \"name\": \"text_editor\",\n",
    "    # Hint: you can write a long description for the tool to help the LLM understand how to use it\n",
    "    \"description\": \"tool description here\",\n",
    "    \"parameters\": {\n",
    "        # Hint: valid type of input parameters are: \"integer\", \"string\", etc\n",
    "        # Hint: don't forget to set the required parameters for the tool\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"command\": {\n",
    "                \"description\": \"The command to execute. Allowed options are: `str_replace`, `line_replace`\",\n",
    "                \"type\": \"string\",\n",
    "                # For command we can use enum to restrict the command to a set of valid options\n",
    "                \"enum\": [\"str_replace\", \"line_replace\"],\n",
    "            },\n",
    "        ...\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "def run_text_edit_command(code, command: str, line_number: int = None, old_str: str = None, new_str: str = None) -> str:\n",
    "    \"\"\"Run the specified command on the code, and return the edited code.\"\"\"\n",
    "    # Hint: you may raise ValueError if missing required parameters or invalid command\n",
    "    if command == \"str_replace\":\n",
    "        # TODO: Implement string replacement\n",
    "        ...\n",
    "    elif command == \"line_replace\":\n",
    "        # TODO: Implement line replacement\n",
    "        ...\n",
    "    else:\n",
    "        raise ValueError(\"Invalid command. Allowed options are: `str_replace`, `line_replace`.\")\n",
    "\n",
    "def call_text_editor(code, command: str, line_number: int = None, old_str: str = None, new_str: str = None) -> ToolResult:\n",
    "    \"\"\"Call the text editor tool to edit the code.\"\"\"\n",
    "    try:\n",
    "        new_code = run_text_edit_command(code, command, line_number, old_str, new_str)\n",
    "        # add the edited code to the data field of the ToolResult\n",
    "        data = {\"new_code\": new_code,}\n",
    "        return ToolResult(output=f\"The code has been successfully edited. Below is the new code:\\n```python\\n{new_code}\\n```\",\n",
    "                          data=data)\n",
    "    except Exception as e:\n",
    "        return ToolError(output=f\"Error: {str(e)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e08864b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_repair_agent(buggy_code: str, client: genai.Client) -> str:\n",
    "    \"\"\"Run the repair agent to fix the buggy code.\"\"\"\n",
    "    chat = client.chats.create(model=\"models/gemini-2.0-flash\")\n",
    "\n",
    "    # TODO: Implement the logic to construct the initial prompt with the buggy code.\n",
    "    user_message = \"...\"\n",
    "\n",
    "    config = create_gemini_config(\n",
    "        max_tokens=512,\n",
    "        temperature=1.0,\n",
    "        batch_size=1,\n",
    "        tools=[types.Tool(functionDeclarations=[text_editor_tool_definition])],\n",
    "    )\n",
    "    response = request_gemini_engine(chat, user_message, config)\n",
    "\n",
    "    # Return the original buggy code if no function calls are made\n",
    "    if not response.function_calls:\n",
    "        return buggy_code\n",
    "    \n",
    "    fixed_code = buggy_code\n",
    "    for function_call in response.function_calls:\n",
    "        func_name = function_call.name\n",
    "        func_args = function_call.args\n",
    "\n",
    "        # TODO: process response and execute the tool call\n",
    "        # Hint: you can use `tool_result.data[\"new_code\"]` to obtain the edited code\n",
    "\n",
    "\n",
    "    return fixed_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3113e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Test the Repair Agent on a single task ###\n",
    "import difflib\n",
    "def get_diff(old_code: str, new_code: str) -> str:\n",
    "    \"\"\"Compute the difference between two code snippets.\"\"\"\n",
    "    diff = difflib.unified_diff(old_code.splitlines(), new_code.splitlines(), fromfile=\"old\", tofile=\"new\", lineterm=\"\")\n",
    "    return \"\\n\".join(diff)\n",
    "\n",
    "fixed_code = run_repair_agent(\n",
    "    buggy_code=buggy_humaneval[0]['buggy_code'],\n",
    "    client=client\n",
    ")\n",
    "print(\"========= Fixed Code =========\")\n",
    "print(fixed_code)\n",
    "print(\"========= DIFF =========\")\n",
    "print(get_diff(buggy_humaneval[0]['buggy_code'], fixed_code))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe57172",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def gemini_repair(client, bug_dataset, workdir) -> float:\n",
    "\n",
    "    for bug in tqdm(bug_dataset):\n",
    "        \n",
    "        # run the repair agent\n",
    "        fixed_code = run_repair_agent(\n",
    "            buggy_code=bug['buggy_code'],\n",
    "            client=client\n",
    "        )\n",
    "\n",
    "        name = bug[\"task_id\"].replace(\"/\", \"_\")\n",
    "        os.makedirs(os.path.join(workdir, name), exist_ok=True)\n",
    "        with open(os.path.join(workdir, name, '0.py'), 'w') as f:\n",
    "            f.write(fixed_code)\n",
    "\n",
    "gemini_repair(client, buggy_humaneval, \"gemini_repair\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d64cf6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!yes Y | evalplus.evaluate --dataset humaneval --samples gemini_repair --i-just-wanna-run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f38e69a",
   "metadata": {},
   "source": [
    "## Repair with test execution feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ec2216",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evalplus.data import get_human_eval_plus\n",
    "human_eval_plus = get_human_eval_plus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181810a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Definition of the RunTestTool\n",
    "run_test_tool_definition = {\n",
    "    \"name\": \"run_test\",\n",
    "    \"description\": \"\"\"A tool that executes a specified function with given test inputs and returns the execution result. \n",
    "This tool is useful for validating the correctness of a function by running it with specific inputs and observing the output.\n",
    "\"\"\".strip(),\n",
    "    \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"function_invocation\": {\n",
    "                \"description\": \"A function invocation string. For example: `add(1, 2)`.\",\n",
    "                \"type\": \"string\",\n",
    "            },\n",
    "        },\n",
    "        \"required\": [\"function_invocation\"],\n",
    "    },\n",
    "}\n",
    "\n",
    "def _execute(function_definition_code: str, function_invocation: str):\n",
    "    \"\"\"Execute the function with the given code and function invocation.\"\"\"\n",
    "    exec_globals = {}\n",
    "    exec_locals = {}\n",
    "    exec(function_definition_code, exec_globals, exec_locals)\n",
    "    result = eval(f\"{function_invocation}\", exec_globals, exec_locals)\n",
    "    return result\n",
    "\n",
    "def call_run_test(code: str, function_invocation: str, canonical_solution: str = None) -> ToolResult:\n",
    "    \"\"\"Run the tests on the code snippet.\"\"\"\n",
    "    expected_output_message = \"\"\n",
    "    if canonical_solution:\n",
    "        try:\n",
    "            gold_result = _execute(canonical_solution, function_invocation)\n",
    "            expected_output_message = f\"\\n\\nExpected output:\\n```\\n{gold_result}\\n```\"\n",
    "        except Exception as e:\n",
    "            return ToolError(output=f\"Error: The test input is not valid.\\n{str(e)}\")\n",
    "    try:\n",
    "        result = _execute(code, function_invocation)\n",
    "        return ToolResult(output=f\"Execution output:\\n```\\n{result}\\n```\" + expected_output_message,\n",
    "                            data={\"result\": result})\n",
    "    except Exception as e:\n",
    "        return ToolError(output=f\"Error: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0a72be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function to dump message to JSON\n",
    "from json import JSONEncoder\n",
    "\n",
    "\n",
    "class MessageEncoder(JSONEncoder):\n",
    "        def default(self, o):\n",
    "            return o.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afbb3aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_repair_with_execution_feedback(client, buggy_code, canonical_solution: str=None) -> str:\n",
    "    chat = client.chats.create(model=\"models/gemini-2.0-flash\")\n",
    "    # Below is an example of how to construct the initial user message\n",
    "    # You can modify it to suit your needs\n",
    "    prompt_template = \"\"\"\n",
    "Please help me fix bugs in the following code snippet with the `text_editor` tool.\n",
    "\n",
    "After fixing the code, you can come up with your own test inputs and test the code with the `run_test` tool.\n",
    "\n",
    "* If the code does not have line numbers, please use the `str_replace` command and avoid using the `line_replace` command.\n",
    "* You can test at most 3 times.\n",
    "\n",
    "```python\n",
    "{buggy_code}\n",
    "```\n",
    "\"\"\".strip()\n",
    "    user_message = prompt_template.format(buggy_code=buggy_code)\n",
    "\n",
    "    config = create_gemini_config(\n",
    "        max_tokens=512,\n",
    "        temperature=1.0,\n",
    "        batch_size=1,\n",
    "        tools=[\n",
    "            types.Tool(functionDeclarations=[text_editor_tool_definition, run_test_tool_definition]),\n",
    "        ],\n",
    "    )\n",
    "    response = request_gemini_engine(chat, user_message, config)\n",
    "\n",
    "    current_code = buggy_code\n",
    "\n",
    "    # Set a maximum number of iterations to prevent infinite loops\n",
    "    for _ in range(20):\n",
    "        # If the model does not return any function calls, we can assume that it has finished\n",
    "        # and we can break the loop\n",
    "        if not response.function_calls:\n",
    "            break\n",
    "        # Now we need to process the function calls\n",
    "        tool_results = []\n",
    "        for function_call in response.function_calls:\n",
    "            func_name = function_call.name\n",
    "            func_args = function_call.args\n",
    "\n",
    "            # TODO: process response and execute the tool call\n",
    "            # Hint 1: pass the `current_code` instead of the original buggy code when executing tools\n",
    "            # Hint 2: don't forget to update the `current_code` when the code is modified\n",
    "        \n",
    "            # tool_result = ...\n",
    "\n",
    "            if isinstance(tool_result, ToolError):\n",
    "                tool_results.append({\n",
    "                    \"resonse\": {\"error\": tool_result.output,},\n",
    "                    \"name\": func_name,\n",
    "                })\n",
    "            else:\n",
    "                tool_results.append({\n",
    "                    \"resonse\": {\"output\": tool_result.output,},\n",
    "                    \"name\": func_name,\n",
    "                })\n",
    "        # Break the loop if no valid tool results are returned\n",
    "        if not tool_results:\n",
    "            break\n",
    "        # Build the tool responses.\n",
    "        # We need to construct a FunctionResponse object for each tool result.\n",
    "        # Check documentation: https://googleapis.github.io/python-genai/genai.html#genai.types.FunctionResponse\n",
    "        # The response field should be a dictionary, use “output” key to specify function output and “error” key to specify error details (if any).\n",
    "        tool_responses = [\n",
    "            types.Part(\n",
    "                function_response=types.FunctionResponse(\n",
    "                    name=tool_result[\"name\"], response=tool_result[\"resonse\"])\n",
    "            )\n",
    "            for tool_result in tool_results\n",
    "        ]\n",
    "        # Send the response parts back to the model\n",
    "        response = request_gemini_engine(chat, tool_responses, config)\n",
    "    return current_code, chat.get_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b24a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Example usage of the repair agent with execution feedback ###\n",
    "task_id = buggy_humaneval[0]['task_id']\n",
    "canonical_solution = human_eval_plus[task_id][\"prompt\"] + human_eval_plus[task_id][\"canonical_solution\"]\n",
    "buggy_code=buggy_humaneval[0]['buggy_code']\n",
    "\n",
    "fixed_code, messages = run_repair_with_execution_feedback(\n",
    "    client=client,\n",
    "    buggy_code=buggy_code,\n",
    "    canonical_solution=canonical_solution,\n",
    ")\n",
    "print(\"========= Fixed Code =========\")\n",
    "print(fixed_code)\n",
    "print(\"========= DIFF =========\")\n",
    "print(get_diff(buggy_humaneval[0]['buggy_code'], fixed_code))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11167c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def gemini_repair(client, bug_dataset, workdir, restart=True) -> float:\n",
    "\n",
    "    for bug in tqdm(bug_dataset):\n",
    "        name = bug[\"task_id\"].replace(\"/\", \"_\")\n",
    "        if not restart:\n",
    "            if os.path.exists(os.path.join(workdir, name, '0.py')):\n",
    "                continue\n",
    "        task_id = bug['task_id']\n",
    "        canonical_solution = human_eval_plus[task_id][\"prompt\"] + human_eval_plus[task_id][\"canonical_solution\"]\n",
    "        \n",
    "        # run the repair agent\n",
    "        fixed_code, messages = run_repair_with_execution_feedback(\n",
    "            client=client,\n",
    "            buggy_code=bug['buggy_code'],\n",
    "            canonical_solution=canonical_solution,\n",
    "        )\n",
    "        name = bug[\"task_id\"].replace(\"/\", \"_\")\n",
    "        os.makedirs(os.path.join(workdir, name), exist_ok=True)\n",
    "        with open(os.path.join(workdir, name, '0.py'), 'w') as f:\n",
    "            f.write(fixed_code)\n",
    "        with open(os.path.join(workdir, name, 'traj.json'), 'w') as f:\n",
    "            json.dump(messages, f, indent=2, cls=MessageEncoder)\n",
    "\n",
    "\n",
    "gemini_repair(client, buggy_humaneval, \"gemini_repair_with_execution_feedback\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2443cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!yes Y | evalplus.evaluate --dataset humaneval --samples gemini_repair_with_execution_feedback --i-just-wanna-run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4006ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
